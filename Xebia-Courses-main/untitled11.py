# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KznU35lgSDu1tt9-n3x6suRQ_TBULimf
"""

# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KznU35lgSDu1tt9-n3x6suRQ_TBULimf
"""

!pip install gensim

from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

sentences = [
    "T am learning natural language processing",
    "Natural language processing is paert of machine learning",
    "I enjoy machine learning",
    "I also enjoy deep learning",
    "Deep learning uses neural networks",
    "Neural networks is powerful for natural language processing"
]
from nltk.tokenize import word_tokenize

# Tokenize
tokenized_sentences = [word_tokenize(s.lower()) for s in sentences]

print(tokenized_sentences)



from gensim.models import Word2Vec

model = Word2Vec(
    sentences=tokenized_sentences,
    vector_size=100,   # embedding size
    window=3,          # context window
    min_count=1        # keep all words
)

print(model.wv.similarity("machine","learning"))

print(model.wv.similarity("learning","deep"))

print(model.wv.most_similar("learning"))

print("Top 5 similar words to machine:")
print(model.wv.most_similar("machine", topn = 5))

# Word2Vec is:
# Shallow neural network
# Only one hidden layer
# Trained using context prediction
# Using Pretrained Embeddings;

import gensim.downloader as api

print("Loading pretrainer model:")
# model = api.load("word2vec-google-news-300")
model = api.load("glove-wiki-gigaword-100")

import gensim.downloader as api

print("Loading pretrainer model:")
model = api.load("word2vec-google-news-300")
# model = api.load("glove-wiki-gigaword-100")

model.similarity("car","bus")

model.most_similar("machine")

#common pretrainer vectors:
# glover-wiki-gigaword-50
# glover-wiki-gigaword-100
# glover-wiki-gigaword-300
# glover-twitter-200

# NLP - Visualizing Embeddings

# We are supposed to reduce dimension from 100D -> 2D
# This will let us visiually see:
# 1. similar words clustered together
# 2. Opposite words separate

# How we are going to reduce Dimensions ?
"""
we will use Dimensionality Reduction:
_ Reduction the number of features while preserving important information

we convert:
10000 -> 100
100 -> 10

why we need to do that ?
To solve problem of Curse of Dimensionality

Types of Dimensionality Reduction:
1. Feature Selection: Remove some useless columns
2. Feature Extraction: Create new columns from existing columns

PCA - Combine features
- finds the direction where data varies most
- these direcions = Principal Components
instead of original features

Suppose user:
height and weight are correlated
Instead of 2 features - PCA creates 1 new feature representing both

How PCA  works?
1. Standardize data
2. Conpute covariance matrix
3. Find eigenvectors
4. Select top components
5. Project data

Suppose we want 2 components:
PC1 -> most information - 60%
PC2 -> second most - 25%

Limitation of PCA:
1: Linear only
2: Cannot capture comples shapes

T-SNE - t-distributed Stdchastic Neighbor Embedding
It preserve:
Local structure (neighbpors)
Points close in high-dim -> close in low-dim
It converts distance -> probabilities
Use KL Divergence

UMAP - Uniform Manifold Approximation


"""

import gensim.downloader as api
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

model = api.load("glove-wiki-gigaword-100")
print("Loading...")

words = [
    "king","queen","man","woman",
    "dog","cat","animal",
    "car","truck","bus", "vehicle",
    "happy","sad","joy","anger"
]

vectors = [model[word] for word in words]

vectors[0].shape

pca = PCA(n_components=2)
reduced = pca.fit_transform(vectors)

reduced.shape

plt.figure(figsize=(10,6))

for i, word in enumerate(words):
  x = reduced[i][0]
  y = reduced[i][1]
  plt.scatter(x,y)
  plt.text(x + 0.01, y + 0.01, word, fontsize=10)

plt.show()

from sklearn.manifold import TSNE
import numpy as np

vectors = np.array([model[word] for word in words])

tsne=TSNE(n_components=2 , perplexity=2)
reduced=tsne.fit_transform(vectors)

plt.figure(figsize=(10,6))

for i, word in enumerate(words):
  x = reduced[i][0]
  y = reduced[i][1]
  plt.scatter(x,y)
  plt.text(x + 0.01, y + 0.01, word)

plt.show()

