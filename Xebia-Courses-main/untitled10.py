# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KznU35lgSDu1tt9-n3x6suRQ_TBULimf
"""

!pip install gensim

from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

sentences = [
    "T am learning natural language processing",
    "Natural language processing is paert of machine learning",
    "I enjoy machine learning",
    "I also enjoy deep learning",
    "Deep learning uses neural networks",
    "Neural networks is powerful for natural language processing"
]

# Tokenize
tokenized_sentences = [word_tokenize(s.lower()) for s in sentences]

print(tokenized_sentences)

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

from gensim.models import Word2Vec

model = Word2Vec(
    sentences=tokenized_sentences,
    vector_size=100,   # embedding size
    window=3,          # context window
    min_count=1        # keep all words
)

print(model.wv.similarity("machine","learning"))

print(model.wv.similarity("learning","deep"))

print(model.wv.most_similar("learning"))

print("Top 5 similar words to machine:")
print(model.wv.most_similar("machine", topn = 5))

# Word2Vec is:
# Shallow neural network
# Only one hidden layer
# Trained using context prediction
# Using Pretrained Embeddings;

import gensim.downloader as api

print("Loading pretrainer model:")
# model = api.load("word2vec-google-news-300")
model = api.load("glove-wiki-gigaword-100")

import gensim.downloader as api

print("Loading pretrainer model:")
model = api.load("word2vec-google-news-300")
# model = api.load("glove-wiki-gigaword-100")

model.similarity("car","bus")

model.most_similar("machine")